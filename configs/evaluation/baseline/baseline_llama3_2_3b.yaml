# @package _global_
defaults:
  - default

experiment_name: "baseline_llama3_2_3b"
run_mode: "baseline"

# Use Llama3.2 3B as the baseline model
baseline_llm:
  type: "ollama"
  model_name: "llama3.2:3b"
  host: "http://localhost:11434"

# Unset router and small_llm as they are not used in baseline mode
router: null
small_llm: null

evaluation:
  output_dir: "results/baselines/llama3_2_3b"
  num_samples: 3500
  seed: 42
  output_file: "results/baselines/llama3_2_3b/baseline_llama3_2_3b_results.json"